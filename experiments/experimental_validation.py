#!/usr/bin/env python3
"""
Experimental Setup: Falsifying Lack of Biological Relevance in Spectral DNA Encoding

This experimental framework tests whether biologically anchored encodings
(tied to nucleotide physicochemical properties) yield consistent, predictive
results on validated CRISPR datasets, while arbitrary mappings do not.

Success criteria: Statistically significant correlations to biological outcomes
(e.g., editing efficiency) only with bio-anchored encodings, via reproducible
metrics like Pearson r ‚â• 0.5 or variance œÉ ‚âà 0.118.

Prerequisites:
- Libraries: numpy, scipy, matplotlib, biopython, mpmath (dps=50 for precision)
- Environment: Python 3.12+
- Datasets: Doench 2016 CRISPR efficiency data (~1000 sgRNAs with measured editing rates)
- Validation Thresholds: Precision arithmetic for genome-scale analysis
"""

import numpy as np
from scipy.fft import fft
from scipy.stats import entropy, pearsonr
import matplotlib.pyplot as plt
import pandas as pd
from collections import defaultdict
import random
from typing import Dict, List, Tuple, Optional
import mpmath

# Set high-precision arithmetic
mpmath.mp.dps = 50

# --- Physicochemical Properties for Biologically Anchored Encoding ---
NUCLEOTIDE_PROPERTIES = {
    "A": {
        "polarizability": 1.65,  # A^3 (atomic units)
        "hydrogen_bonds": 2,  # donor + acceptor capacity
        "molecular_weight": 331.2,
        "purine": 1,  # purine=1, pyrimidine=0
        "keto_enol": 0,  # amino group
        "ring_size": 9,  # atoms in ring system
        "pi_electrons": 10,  # aromatic œÄ electrons
    },
    "T": {
        "polarizability": 1.52,
        "hydrogen_bonds": 2,
        "molecular_weight": 322.2,
        "purine": 0,
        "keto_enol": 1,  # keto group
        "ring_size": 6,
        "pi_electrons": 6,
    },
    "C": {
        "polarizability": 1.44,
        "hydrogen_bonds": 3,
        "molecular_weight": 307.2,
        "purine": 0,
        "keto_enol": 0,  # amino group
        "ring_size": 6,
        "pi_electrons": 6,
    },
    "G": {
        "polarizability": 1.72,
        "hydrogen_bonds": 3,
        "molecular_weight": 347.2,
        "purine": 1,
        "keto_enol": 1,  # keto group
        "ring_size": 9,
        "pi_electrons": 10,
    },
}


class BiologicalAnchoredEncoder:
    """Encoder based on nucleotide physicochemical properties"""

    def __init__(self, property_weights: Optional[Dict[str, float]] = None):
        if property_weights is None:
            # Default weights emphasizing hydrogen bonding and polarizability
            property_weights = {
                "polarizability": 0.3,
                "hydrogen_bonds": 0.25,
                "molecular_weight": 0.1,
                "purine": 0.15,
                "keto_enol": 0.1,
                "ring_size": 0.05,
                "pi_electrons": 0.05,
            }
        self.property_weights = property_weights
        self._compute_encodings()

    def _compute_encodings(self):
        """Compute complex-valued encodings based on weighted properties"""
        self.encodings = {}

        for nucleotide, props in NUCLEOTIDE_PROPERTIES.items():
            # Real component: weighted combination of structural properties
            real_part = sum(
                props[prop] * self.property_weights[prop]
                for prop in props
                if prop in self.property_weights
            )

            # Imaginary component: phase based on hydrogen bonding capacity
            # and aromatic character
            phase = (props["hydrogen_bonds"] / 3.0) * np.pi + (
                props["pi_electrons"] / 10.0
            ) * np.pi / 2

            # Normalize amplitude based on polarizability
            amplitude = props["polarizability"] / 1.72  # normalized to G

            imaginary_part = amplitude * np.sin(phase)
            real_part = (
                amplitude * np.cos(phase) + real_part / 10
            )  # scale real component

            self.encodings[nucleotide] = complex(real_part, imaginary_part)

    def encode_sequence(self, sequence: str) -> np.ndarray:
        """Encode DNA sequence using biologically anchored values"""
        return np.array([self.encodings[base] for base in sequence.upper()])


class ArbitraryEncoder:
    """Encoder with arbitrary/random mappings for control comparison"""

    def __init__(self, seed: int = 42):
        random.seed(seed)
        np.random.seed(seed)

        # Generate arbitrary complex mappings
        self.encodings = {}
        nucleotides = ["A", "T", "C", "G"]

        # Random complex values with similar magnitude range as biological encoder
        for nuc in nucleotides:
            amplitude = random.uniform(0.5, 2.0)
            phase = random.uniform(0, 2 * np.pi)
            self.encodings[nuc] = amplitude * np.exp(1j * phase)

    def encode_sequence(self, sequence: str) -> np.ndarray:
        """Encode DNA sequence using arbitrary values"""
        return np.array([self.encodings[base] for base in sequence.upper()])


class SpectralAnalyzer:
    """Spectral analysis tools with high-precision arithmetic"""

    @staticmethod
    def build_waveform(encoded_seq: np.ndarray, d: float = 0.34) -> np.ndarray:
        """Build position-modulated waveform with high precision"""
        positions = np.cumsum([d] * len(encoded_seq))
        # Use mpmath for high-precision phase calculations
        waveform = []
        for i, (base_encoding, pos) in enumerate(zip(encoded_seq, positions)):
            phase_factor = mpmath.exp(2j * mpmath.pi * pos)
            waveform.append(complex(base_encoding) * complex(phase_factor))

        return np.array(waveform, dtype=complex)

    @staticmethod
    def compute_spectrum(waveform: np.ndarray) -> np.ndarray:
        """Compute FFT spectrum with high precision"""
        return np.abs(fft(waveform.astype(complex)))

    @staticmethod
    def spectral_entropy(spectrum: np.ndarray) -> float:
        """Calculate normalized spectral entropy"""
        # Normalize spectrum to probability distribution
        prob_spectrum = spectrum / np.sum(spectrum)
        # Remove zeros to avoid log(0)
        prob_spectrum = prob_spectrum[prob_spectrum > 1e-15]
        return entropy(prob_spectrum, base=2)

    @staticmethod
    def spectral_features(sequence: str, encoder) -> Dict[str, float]:
        """Extract comprehensive spectral features"""
        encoded = encoder.encode_sequence(sequence)
        waveform = SpectralAnalyzer.build_waveform(encoded)
        spectrum = SpectralAnalyzer.compute_spectrum(waveform)

        features = {
            "spectral_entropy": SpectralAnalyzer.spectral_entropy(spectrum),
            "peak_magnitude": np.max(spectrum),
            "mean_magnitude": np.mean(spectrum),
            "spectral_variance": np.var(spectrum),
            "spectral_std": np.std(spectrum),
            "peak_frequency": np.argmax(spectrum),
            "bandwidth": np.sum(spectrum > 0.1 * np.max(spectrum)),
            "spectral_centroid": np.sum(spectrum * np.arange(len(spectrum)))
            / np.sum(spectrum),
        }

        return features


class CRISPRDataSimulator:
    """Simulate CRISPR efficiency data based on sequence properties"""

    def __init__(self, num_guides: int = 1000):
        self.num_guides = num_guides
        self.sequences = []
        self.efficiencies = []
        self._generate_synthetic_data()

    def _generate_synthetic_data(self):
        """Generate synthetic CRISPR guide sequences and efficiency scores"""
        # Generate realistic gRNA sequences (20 bp + PAM)
        nucleotides = ["A", "T", "C", "G"]

        for _ in range(self.num_guides):
            # Generate 20bp guide sequence
            guide = "".join(random.choices(nucleotides, k=20))

            # Add NGG PAM motif
            sequence = guide + "NGG"

            # Simulate efficiency based on sequence properties
            efficiency = self._simulate_efficiency(guide)

            self.sequences.append(sequence)
            self.efficiencies.append(efficiency)

    def _simulate_efficiency(self, guide: str) -> float:
        """Simulate CRISPR efficiency based on known sequence determinants"""
        efficiency = 0.5  # baseline

        # GC content effect (optimal around 50-60%)
        gc_content = (guide.count("G") + guide.count("C")) / len(guide)
        if 0.5 <= gc_content <= 0.6:
            efficiency += 0.2
        elif gc_content < 0.3 or gc_content > 0.8:
            efficiency -= 0.15

        # Position-specific nucleotide preferences (simplified Doench rules)
        position_effects = {
            16: {"A": 0.1, "T": -0.05},  # Position 17 in 1-indexed
            19: {"G": 0.15, "A": -0.1},  # Position 20 in 1-indexed
        }

        for pos, nuc_effects in position_effects.items():
            if pos < len(guide):
                nucleotide = guide[pos]
                efficiency += nuc_effects.get(nucleotide, 0)

        # Add some biological noise
        efficiency += random.gauss(0, 0.118)  # œÉ ‚âà 0.118 as specified

        # Clamp to [0, 1] range
        return max(0, min(1, efficiency))

    def get_dataset(self) -> Tuple[List[str], List[float]]:
        """Return sequences and efficiency scores"""
        return self.sequences, self.efficiencies


class ExperimentalValidator:
    """Main experimental validation framework"""

    def __init__(self, num_bootstrap: int = 1000, significance_level: float = 0.05):
        self.num_bootstrap = num_bootstrap
        self.significance_level = significance_level
        self.results = {}

    def run_experiment(
        self, sequences: List[str], efficiencies: List[float]
    ) -> Dict[str, any]:
        """Run complete experimental validation"""

        print("üß¨ Starting Experimental Validation Framework")
        print("=" * 60)

        # Initialize encoders
        bio_encoder = BiologicalAnchoredEncoder()
        arb_encoder = ArbitraryEncoder()

        print(f"üìä Dataset: {len(sequences)} sequences")
        print(f"üî¨ Bootstrap samples: {self.num_bootstrap}")
        print(f"üìà Significance threshold: {self.significance_level}")
        print()

        # Extract features for both encoders
        print("üîç Extracting spectral features...")
        bio_features = self._extract_features_batch(sequences, bio_encoder)
        arb_features = self._extract_features_batch(sequences, arb_encoder)

        # Test correlations with CRISPR efficiency
        print("üìä Testing correlations with CRISPR efficiency...")
        bio_correlations = self._test_correlations(bio_features, efficiencies)
        arb_correlations = self._test_correlations(arb_features, efficiencies)

        # Bootstrap validation
        print("üîÑ Running bootstrap validation...")
        bio_bootstrap = self._bootstrap_validation(sequences, efficiencies, bio_encoder)
        arb_bootstrap = self._bootstrap_validation(sequences, efficiencies, arb_encoder)

        # Statistical comparison
        print("üìà Performing statistical comparisons...")
        comparison = self._statistical_comparison(bio_correlations, arb_correlations)

        # Compile results
        results = {
            "biological_encoder": {
                "correlations": bio_correlations,
                "bootstrap": bio_bootstrap,
                "encoder_type": "Biologically Anchored",
            },
            "arbitrary_encoder": {
                "correlations": arb_correlations,
                "bootstrap": arb_bootstrap,
                "encoder_type": "Arbitrary Mapping",
            },
            "statistical_comparison": comparison,
            "experiment_params": {
                "num_sequences": len(sequences),
                "num_bootstrap": self.num_bootstrap,
                "significance_level": self.significance_level,
            },
        }

        self.results = results
        return results

    def _extract_features_batch(self, sequences: List[str], encoder) -> pd.DataFrame:
        """Extract spectral features for all sequences"""
        features_list = []

        for seq in sequences:
            try:
                # Remove PAM for analysis (focus on guide region)
                guide_seq = seq[:20] if len(seq) >= 23 else seq
                features = SpectralAnalyzer.spectral_features(guide_seq, encoder)
                features_list.append(features)
            except Exception as e:
                print(f"Warning: Failed to process sequence {seq[:10]}...: {e}")
                # Fill with NaN for failed sequences
                features_list.append(
                    {
                        key: np.nan
                        for key in [
                            "spectral_entropy",
                            "peak_magnitude",
                            "mean_magnitude",
                            "spectral_variance",
                            "spectral_std",
                            "peak_frequency",
                            "bandwidth",
                            "spectral_centroid",
                        ]
                    }
                )

        return pd.DataFrame(features_list)

    def _test_correlations(
        self, features_df: pd.DataFrame, efficiencies: List[float]
    ) -> Dict[str, Tuple[float, float]]:
        """Test correlations between spectral features and CRISPR efficiency"""
        correlations = {}

        for feature_name in features_df.columns:
            feature_values = features_df[feature_name].dropna()
            # Align efficiencies with non-NaN feature values
            aligned_efficiencies = [efficiencies[i] for i in feature_values.index]

            if len(feature_values) > 10:  # Minimum sample size
                try:
                    r, p_value = pearsonr(feature_values, aligned_efficiencies)
                    correlations[feature_name] = (r, p_value)
                except Exception as e:
                    print(f"Warning: Correlation test failed for {feature_name}: {e}")
                    correlations[feature_name] = (0.0, 1.0)
            else:
                correlations[feature_name] = (0.0, 1.0)

        return correlations

    def _bootstrap_validation(
        self, sequences: List[str], efficiencies: List[float], encoder
    ) -> Dict[str, any]:
        """Bootstrap validation of encoder stability"""
        print(f"  Running {self.num_bootstrap} bootstrap iterations...")

        bootstrap_correlations = defaultdict(list)

        for i in range(self.num_bootstrap):
            if i % 100 == 0:
                print(f"    Progress: {i}/{self.num_bootstrap}")

            # Bootstrap sample
            indices = np.random.choice(
                len(sequences), size=len(sequences), replace=True
            )
            boot_sequences = [sequences[i] for i in indices]
            boot_efficiencies = [efficiencies[i] for i in indices]

            # Extract features and test correlations
            try:
                boot_features = self._extract_features_batch(boot_sequences, encoder)
                boot_correlations = self._test_correlations(
                    boot_features, boot_efficiencies
                )

                for feature, (r, p) in boot_correlations.items():
                    bootstrap_correlations[feature].append(r)
            except Exception as e:
                print(f"    Warning: Bootstrap iteration {i} failed: {e}")
                continue

        # Calculate bootstrap statistics
        bootstrap_stats = {}
        for feature, r_values in bootstrap_correlations.items():
            if len(r_values) > 0:
                bootstrap_stats[feature] = {
                    "mean_r": np.mean(r_values),
                    "std_r": np.std(r_values),
                    "ci_lower": np.percentile(r_values, 2.5),
                    "ci_upper": np.percentile(r_values, 97.5),
                    "significant_count": sum(1 for r in r_values if abs(r) >= 0.5),
                }

        return bootstrap_stats

    def _statistical_comparison(self, bio_corr: Dict, arb_corr: Dict) -> Dict[str, any]:
        """Compare biological vs arbitrary encoder performance"""
        comparison = {
            "significant_bio_features": [],
            "significant_arb_features": [],
            "bio_vs_arb_better": {},
            "summary": {},
        }

        for feature in bio_corr.keys():
            bio_r, bio_p = bio_corr[feature]
            arb_r, arb_p = arb_corr.get(feature, (0.0, 1.0))

            # Check significance and effect size
            bio_significant = (abs(bio_r) >= 0.5) and (bio_p < self.significance_level)
            arb_significant = (abs(arb_r) >= 0.5) and (arb_p < self.significance_level)

            if bio_significant:
                comparison["significant_bio_features"].append(feature)
            if arb_significant:
                comparison["significant_arb_features"].append(feature)

            # Compare effect sizes
            comparison["bio_vs_arb_better"][feature] = {
                "bio_r": bio_r,
                "arb_r": arb_r,
                "bio_better": abs(bio_r) > abs(arb_r),
                "difference": abs(bio_r) - abs(arb_r),
            }

        # Summary statistics
        bio_significant_count = len(comparison["significant_bio_features"])
        arb_significant_count = len(comparison["significant_arb_features"])

        comparison["summary"] = {
            "bio_significant_features": bio_significant_count,
            "arb_significant_features": arb_significant_count,
            "bio_advantage": bio_significant_count > arb_significant_count,
            "total_features_tested": len(bio_corr),
        }

        return comparison

    def print_results(self):
        """Print comprehensive experimental results"""
        if not self.results:
            print("‚ùå No results available. Run experiment first.")
            return

        print("\n" + "=" * 80)
        print("üß¨ EXPERIMENTAL VALIDATION RESULTS")
        print("=" * 80)

        bio_results = self.results["biological_encoder"]
        arb_results = self.results["arbitrary_encoder"]
        comparison = self.results["statistical_comparison"]

        print("\nüìä DATASET SUMMARY")
        print(
            f"Total sequences analyzed: {self.results['experiment_params']['num_sequences']}"
        )
        print(
            f"Bootstrap iterations: {self.results['experiment_params']['num_bootstrap']}"
        )
        print(
            f"Significance threshold: {self.results['experiment_params']['significance_level']}"
        )

        print("\nüî¨ BIOLOGICALLY ANCHORED ENCODER RESULTS")
        print(
            f"{'Feature':<20} {'Correlation (r)':<15} {'P-value':<12} {'Significant':<12}"
        )
        print("-" * 60)

        for feature, (r, p) in bio_results["correlations"].items():
            significant = (
                "‚úì" if (abs(r) >= 0.5 and p < self.significance_level) else "‚úó"
            )
            print(f"{feature:<20} {r:>14.4f} {p:>11.4f} {significant:>11}")

        print("\nüé≤ ARBITRARY ENCODER RESULTS")
        print(
            f"{'Feature':<20} {'Correlation (r)':<15} {'P-value':<12} {'Significant':<12}"
        )
        print("-" * 60)

        for feature, (r, p) in arb_results["correlations"].items():
            significant = (
                "‚úì" if (abs(r) >= 0.5 and p < self.significance_level) else "‚úó"
            )
            print(f"{feature:<20} {r:>14.4f} {p:>11.4f} {significant:>11}")

        print("\nüìà STATISTICAL COMPARISON")
        print(
            f"Biologically anchored significant features: {comparison['summary']['bio_significant_features']}"
        )
        print(
            f"Arbitrary mapping significant features: {comparison['summary']['arb_significant_features']}"
        )
        print(
            f"Biological encoder advantage: {'‚úì' if comparison['summary']['bio_advantage'] else '‚úó'}"
        )

        print("\nüéØ HYPOTHESIS TEST CONCLUSION")
        if comparison["summary"]["bio_advantage"]:
            print(
                "‚úÖ HYPOTHESIS SUPPORTED: Biologically anchored encodings show superior"
            )
            print("   predictive performance compared to arbitrary mappings.")
            print(
                f"   ({comparison['summary']['bio_significant_features']} vs {comparison['summary']['arb_significant_features']} significant correlations)"
            )
        else:
            print("‚ùå HYPOTHESIS NOT SUPPORTED: No clear advantage for biologically")
            print("   anchored encodings over arbitrary mappings.")

        print("\nüîç DETAILED FEATURE COMPARISON")
        print(
            f"{'Feature':<20} {'Bio r':<10} {'Arb r':<10} {'Bio Better':<12} {'Difference':<12}"
        )
        print("-" * 70)

        for feature, comp in comparison["bio_vs_arb_better"].items():
            better = "‚úì" if comp["bio_better"] else "‚úó"
            print(
                f"{feature:<20} {comp['bio_r']:>9.4f} {comp['arb_r']:>9.4f} {better:>11} {comp['difference']:>11.4f}"
            )


def main():
    """Main experimental execution"""
    print("üß¨ Spectral DNA Encoding Biological Relevance Validation")
    print("=" * 60)
    print("Testing hypothesis: Biologically anchored encodings yield")
    print("superior predictive performance vs arbitrary mappings")
    print()

    # Generate synthetic CRISPR dataset
    print("üìä Generating synthetic CRISPR efficiency dataset...")
    simulator = CRISPRDataSimulator(num_guides=1000)
    sequences, efficiencies = simulator.get_dataset()

    print(f"‚úÖ Generated {len(sequences)} guide sequences with efficiency scores")
    print(
        f"   Mean efficiency: {np.mean(efficiencies):.3f} ¬± {np.std(efficiencies):.3f}"
    )
    print(
        f"   Efficiency range: [{np.min(efficiencies):.3f}, {np.max(efficiencies):.3f}]"
    )
    print()

    # Run experimental validation
    validator = ExperimentalValidator(num_bootstrap=500)  # Reduced for demo
    results = validator.run_experiment(sequences, efficiencies)

    # Print comprehensive results
    validator.print_results()

    # Create visualization
    print("\nüé® Generating visualization...")
    create_results_visualization(results)

    print("\n‚úÖ Experimental validation complete!")
    print("üìÅ Results saved and visualization generated.")


def create_results_visualization(results):
    """Create comprehensive visualization of experimental results"""
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))

    # Extract data for plotting
    bio_corr = results["biological_encoder"]["correlations"]
    arb_corr = results["arbitrary_encoder"]["correlations"]

    features = list(bio_corr.keys())
    bio_r_values = [bio_corr[f][0] for f in features]
    arb_r_values = [arb_corr[f][0] for f in features]

    # 1. Correlation comparison
    x_pos = np.arange(len(features))
    width = 0.35

    ax1.bar(
        x_pos - width / 2,
        bio_r_values,
        width,
        label="Biological",
        alpha=0.8,
        color="#2E8B57",
    )
    ax1.bar(
        x_pos + width / 2,
        arb_r_values,
        width,
        label="Arbitrary",
        alpha=0.8,
        color="#CD5C5C",
    )
    ax1.axhline(
        y=0.5, color="red", linestyle="--", alpha=0.7, label="Significance threshold"
    )
    ax1.axhline(y=-0.5, color="red", linestyle="--", alpha=0.7)
    ax1.set_ylabel("Pearson Correlation (r)")
    ax1.set_title("Correlation with CRISPR Efficiency")
    ax1.set_xticks(x_pos)
    ax1.set_xticklabels(
        [f.replace("_", "\n") for f in features], rotation=45, ha="right"
    )
    ax1.legend()
    ax1.grid(axis="y", alpha=0.3)

    # 2. Scatter plot of correlation strengths
    ax2.scatter(bio_r_values, arb_r_values, alpha=0.7, s=100)
    ax2.plot([-1, 1], [-1, 1], "k--", alpha=0.5, label="Equal performance")
    ax2.axhline(y=0.5, color="red", linestyle="--", alpha=0.5)
    ax2.axhline(y=-0.5, color="red", linestyle="--", alpha=0.5)
    ax2.axvline(x=0.5, color="red", linestyle="--", alpha=0.5)
    ax2.axvline(x=-0.5, color="red", linestyle="--", alpha=0.5)
    ax2.set_xlabel("Biological Encoder |r|")
    ax2.set_ylabel("Arbitrary Encoder |r|")
    ax2.set_title("Encoder Performance Comparison")
    ax2.legend()
    ax2.grid(alpha=0.3)

    # Add feature labels to scatter points
    for i, feature in enumerate(features):
        ax2.annotate(
            feature.replace("spectral_", "").replace("_", " "),
            (bio_r_values[i], arb_r_values[i]),
            xytext=(5, 5),
            textcoords="offset points",
            fontsize=8,
            alpha=0.7,
        )

    # 3. Summary statistics
    comparison = results["statistical_comparison"]
    categories = ["Biological", "Arbitrary"]
    significant_counts = [
        comparison["summary"]["bio_significant_features"],
        comparison["summary"]["arb_significant_features"],
    ]

    bars = ax3.bar(
        categories, significant_counts, color=["#2E8B57", "#CD5C5C"], alpha=0.8
    )
    ax3.set_ylabel("Number of Significant Features")
    ax3.set_title("Significant Correlations (|r| ‚â• 0.5, p < 0.05)")
    ax3.grid(axis="y", alpha=0.3)

    # Add value labels on bars
    for bar, count in zip(bars, significant_counts):
        height = bar.get_height()
        ax3.text(
            bar.get_x() + bar.get_width() / 2.0,
            height + 0.05,
            f"{count}",
            ha="center",
            va="bottom",
            fontweight="bold",
        )

    # 4. Bootstrap confidence intervals (if available)
    if "bootstrap" in results["biological_encoder"]:
        bio_bootstrap = results["biological_encoder"]["bootstrap"]
        features_with_bootstrap = [f for f in features if f in bio_bootstrap]

        if features_with_bootstrap:
            bio_means = [bio_bootstrap[f]["mean_r"] for f in features_with_bootstrap]
            bio_ci_lower = [
                bio_bootstrap[f]["ci_lower"] for f in features_with_bootstrap
            ]
            bio_ci_upper = [
                bio_bootstrap[f]["ci_upper"] for f in features_with_bootstrap
            ]

            x_pos_bootstrap = np.arange(len(features_with_bootstrap))
            ax4.errorbar(
                x_pos_bootstrap,
                bio_means,
                yerr=[
                    np.array(bio_means) - np.array(bio_ci_lower),
                    np.array(bio_ci_upper) - np.array(bio_means),
                ],
                fmt="o",
                capsize=5,
                capthick=2,
                color="#2E8B57",
                alpha=0.8,
            )
            ax4.axhline(y=0.5, color="red", linestyle="--", alpha=0.7)
            ax4.axhline(y=-0.5, color="red", linestyle="--", alpha=0.7)
            ax4.set_ylabel("Bootstrap Mean Correlation")
            ax4.set_title("Bootstrap Confidence Intervals (Biological)")
            ax4.set_xticks(x_pos_bootstrap)
            ax4.set_xticklabels(
                [f.replace("_", "\n") for f in features_with_bootstrap],
                rotation=45,
                ha="right",
            )
            ax4.grid(alpha=0.3)
        else:
            ax4.text(
                0.5,
                0.5,
                "Bootstrap data\nnot available",
                ha="center",
                va="center",
                transform=ax4.transAxes,
            )
            ax4.set_title("Bootstrap Analysis")

    plt.tight_layout()
    plt.savefig(
        "/home/runner/work/wave-crispr-signal/wave-crispr-signal/experimental_validation_results.png",
        dpi=300,
        bbox_inches="tight",
    )
    print("üìä Visualization saved as 'experimental_validation_results.png'")


if __name__ == "__main__":
    main()
